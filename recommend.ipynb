{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ex3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfDjK8Pvvukz",
        "outputId": "389d696b-f0d3-4789-e124-6bbb170813e0"
      },
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install -q -q -U tqdm scikit-learn pandas sparsesvd plotly\n",
        "\n",
        "# install plotting dependencies\n",
        "!wget -q https://github.com/plotly/orca/releases/download/v1.2.1/orca-1.2.1-x86_64.AppImage -O /usr/local/bin/orca\n",
        "!chmod +x /usr/local/bin/orca\n",
        "!apt-get -qq install xvfb libgtk2.0-0 libgconf-2-4"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 81kB 8.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 22.2MB 1.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 13.2MB 254kB/s \n",
            "\u001b[?25h  Building wheel for sparsesvd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Selecting previously unselected package libdbus-glib-1-2:amd64.\n",
            "(Reading database ... 146442 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libdbus-glib-1-2_0.110-2_amd64.deb ...\n",
            "Unpacking libdbus-glib-1-2:amd64 (0.110-2) ...\n",
            "Selecting previously unselected package gconf2-common.\n",
            "Preparing to unpack .../01-gconf2-common_3.2.6-4ubuntu1_all.deb ...\n",
            "Unpacking gconf2-common (3.2.6-4ubuntu1) ...\n",
            "Selecting previously unselected package libgconf-2-4:amd64.\n",
            "Preparing to unpack .../02-libgconf-2-4_3.2.6-4ubuntu1_amd64.deb ...\n",
            "Unpacking libgconf-2-4:amd64 (3.2.6-4ubuntu1) ...\n",
            "Selecting previously unselected package gconf-service-backend.\n",
            "Preparing to unpack .../03-gconf-service-backend_3.2.6-4ubuntu1_amd64.deb ...\n",
            "Unpacking gconf-service-backend (3.2.6-4ubuntu1) ...\n",
            "Selecting previously unselected package gconf-service.\n",
            "Preparing to unpack .../04-gconf-service_3.2.6-4ubuntu1_amd64.deb ...\n",
            "Unpacking gconf-service (3.2.6-4ubuntu1) ...\n",
            "Selecting previously unselected package libgtk2.0-common.\n",
            "Preparing to unpack .../05-libgtk2.0-common_2.24.32-1ubuntu1_all.deb ...\n",
            "Unpacking libgtk2.0-common (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package libgtk2.0-0:amd64.\n",
            "Preparing to unpack .../06-libgtk2.0-0_2.24.32-1ubuntu1_amd64.deb ...\n",
            "Unpacking libgtk2.0-0:amd64 (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package libgail18:amd64.\n",
            "Preparing to unpack .../07-libgail18_2.24.32-1ubuntu1_amd64.deb ...\n",
            "Unpacking libgail18:amd64 (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package libgail-common:amd64.\n",
            "Preparing to unpack .../08-libgail-common_2.24.32-1ubuntu1_amd64.deb ...\n",
            "Unpacking libgail-common:amd64 (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package libgtk2.0-bin.\n",
            "Preparing to unpack .../09-libgtk2.0-bin_2.24.32-1ubuntu1_amd64.deb ...\n",
            "Unpacking libgtk2.0-bin (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../10-xvfb_2%3a1.19.6-1ubuntu4.8_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Setting up gconf2-common (3.2.6-4ubuntu1) ...\n",
            "\n",
            "Creating config file /etc/gconf/2/path with new version\n",
            "Setting up libgtk2.0-common (2.24.32-1ubuntu1) ...\n",
            "Setting up libdbus-glib-1-2:amd64 (0.110-2) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Setting up libgconf-2-4:amd64 (3.2.6-4ubuntu1) ...\n",
            "Setting up libgtk2.0-0:amd64 (2.24.32-1ubuntu1) ...\n",
            "Setting up libgail18:amd64 (2.24.32-1ubuntu1) ...\n",
            "Setting up libgail-common:amd64 (2.24.32-1ubuntu1) ...\n",
            "Setting up libgtk2.0-bin (2.24.32-1ubuntu1) ...\n",
            "Setting up gconf-service-backend (3.2.6-4ubuntu1) ...\n",
            "Setting up gconf-service (3.2.6-4ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjlXMCUXvuk5"
      },
      "source": [
        "import datetime, os, random, shutil, urllib.request, zipfile, time, warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from functools import wraps\n",
        "from math import trunc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from urllib.request import urlopen\n",
        "from zipfile import ZipFile\n",
        "from scipy.sparse.linalg import norm\n",
        "import scipy.sparse as ss\n",
        "from scipy.sparse.linalg import svds\n",
        "from sparsesvd import sparsesvd\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "import plotly.express as px\n",
        "\n",
        "\n",
        "SEED = 123\n",
        "def seed_everything(seed=SEED):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "seed_everything()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mktkcXOFQrw"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lj_Na91f60Vw"
      },
      "source": [
        "def get_dataset():\n",
        "    path = Path(\"m1.zip\")\n",
        "    if not path.exists():\n",
        "        with path.open(\"wb\") as f:\n",
        "            print(\"Downloading dataset...\")\n",
        "            f.write(urlopen(\"http://files.grouplens.org/datasets/movielens/ml-1m.zip\").read())\n",
        "    if not Path(\"ml-1m\").is_dir():\n",
        "        print(\"unzipping...\")\n",
        "        with ZipFile(\"m1.zip\") as zf:\n",
        "            zf.extractall()\n",
        "    ratings_list = [i.strip().split(\"::\") for i in open('ml-1m/ratings.dat', 'r').readlines()]\n",
        "    ratings_df = pd.DataFrame(ratings_list, columns = ['UserID', 'MovieID', 'Rating', 'Timestamp'], dtype = int)\n",
        "    ratings_df['Rating'] = ratings_df['Rating'].apply(pd.to_numeric)\n",
        "    return ratings_df"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iI9441n7DltF"
      },
      "source": [
        "def split(df):\n",
        "    R_df = ratings_df.pivot(index = 'UserID', columns ='MovieID', values = 'Rating').fillna(0)\n",
        "    seed_everything()\n",
        "    test_indices = np.array(random.sample(list(np.argwhere(R_df.values>0)), 2500))\n",
        "\n",
        "    x_indices = test_indices[:,0]\n",
        "    y_indices = test_indices[:,1]\n",
        "    u_ids = R_df.index[x_indices].astype(np.int32)\n",
        "    i_ids = R_df.columns[y_indices].astype(np.int32)\n",
        "    \n",
        "    df['UserID'] = df['UserID'].astype(np.int32)\n",
        "    df['MovieID'] = df['MovieID'].astype(np.int32)\n",
        "    df['Rating'] = df['Rating'].astype(np.float64)\n",
        "    \n",
        "    df= df.drop(columns=['Timestamp'], errors='ignore')\n",
        "    \n",
        "    test = []\n",
        "    for u_id, i_id in zip(u_ids, i_ids):\n",
        "        test.append(df.loc[(df['UserID'] == u_id) & (df['MovieID'] == i_id)])\n",
        "    test_df = pd.concat(test)\n",
        "    train_df= df.drop(test_df.index.tolist())\n",
        "    \n",
        "    \n",
        "    matrix_only_with_test = np.zeros(R_df.shape, dtype=np.float64) \n",
        "    matrix_only_with_test[x_indices, y_indices] =  R_df.values[x_indices, y_indices]\n",
        "    M_test_df = pd.DataFrame(matrix_only_with_test, index=R_df.index, columns=R_df.columns)\n",
        "    \n",
        "    matrix_without_test = R_df.copy().values\n",
        "    matrix_without_test[x_indices,y_indices] = 0.0\n",
        "    M_train_df = pd.DataFrame(matrix_without_test, index=R_df.index, columns=R_df.columns)\n",
        "    \n",
        "    return train_df, test_df, M_train_df, M_test_df\n",
        "    "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6PSReAzrbvQ",
        "outputId": "100fb7d9-0e5a-4eae-d4e2-8e933536152f"
      },
      "source": [
        "ratings_df = get_dataset()\n",
        "train_df, test_df, M_train_df, M_test_df = split(ratings_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading dataset...\n",
            "unzipping...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOfa3VEpvuk7"
      },
      "source": [
        "test_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kF_se-_Gvuk8"
      },
      "source": [
        "M_train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2rW5teNvuk8"
      },
      "source": [
        "\n",
        "\n",
        "# Funk-SVD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dACU8JFUvuk9"
      },
      "source": [
        "class FunkSVD:\n",
        "    def __init__(self, lr=.005, reg=.02, n_epochs=20, n_factors=100,\n",
        "                 min_delta=.001,\n",
        "                 min_rating=1, max_rating=5):\n",
        "\n",
        "        self.lr = lr\n",
        "        self.reg = reg\n",
        "        self.n_epochs = n_epochs\n",
        "        self.n_factors = n_factors\n",
        "        self.min_rating = min_rating\n",
        "        self.max_rating = max_rating\n",
        "\n",
        "    def fit(self, X, X_test):\n",
        "        X = self._preprocess_data(X)\n",
        "\n",
        "        \n",
        "        X_test = self._preprocess_data(X_test, train=False)\n",
        "        self._init_metrics()\n",
        "        self.global_mean_ = np.mean(X[:, 2])\n",
        "        return self._run_sgd(X, X_test)\n",
        "\n",
        "    def _preprocess_data(self, X, train=True):\n",
        "        X = X.copy()\n",
        "        if train:  # Mappings have to be created\n",
        "            #assumed that train data includes all possible users and movies (not necessarily ratings)\n",
        "            user_ids = X['UserID'].unique().tolist()\n",
        "            item_ids = X['MovieID'].unique().tolist()\n",
        "            n_users = len(user_ids)\n",
        "            n_items = len(item_ids)\n",
        "            user_idx = range(n_users)\n",
        "            item_idx = range(n_items)\n",
        "            self.user_mapping_ = dict(zip(user_ids, user_idx))\n",
        "            self.item_mapping_ = dict(zip(item_ids, item_idx))\n",
        "        X['UserID'] = X['UserID'].map(self.user_mapping_)\n",
        "        X['MovieID'] = X['MovieID'].map(self.item_mapping_)\n",
        "        \n",
        "        X['UserID'] = X['UserID'].astype(np.int32)\n",
        "        X['MovieID'] = X['MovieID'].astype(np.int32)\n",
        "        return X[['UserID', 'MovieID', 'Rating']].values\n",
        "\n",
        "    def _init_metrics(self):\n",
        "        metrics = np.zeros((self.n_epochs, 3), dtype=np.float)\n",
        "        self.metrics_ = pd.DataFrame(metrics, columns=['Loss', 'RMSE', 'MAE'])\n",
        "\n",
        "    def _run_sgd(self, X, X_test):\n",
        "        reg,lr,global_mean,n_factors = self.reg,self.lr,self.global_mean_,self.n_factors\n",
        "        \n",
        "        n_users = len(np.unique(X[:, 0]))\n",
        "        n_items = len(np.unique(X[:, 1]))\n",
        "        bu = np.zeros(n_users)\n",
        "        bi = np.zeros(n_items)\n",
        "        pu = np.random.normal(0, .1, (n_users, n_factors))\n",
        "        qi = np.random.normal(0, .1, (n_items, n_factors))\n",
        "        \n",
        "        indices = (X_test[:,0].astype(int), X_test[:,1].astype(int))\n",
        "        true = X_test[:,2]\n",
        "        \n",
        "        # Run SGD\n",
        "        pbar = tqdm(range(self.n_epochs), desc='Epoch',\n",
        "                             ncols=110)\n",
        "        \n",
        "        #stochastic sgd\n",
        "        for epoch_ix in pbar:\n",
        "            \n",
        "            #with batch size 1\n",
        "            for row in X:\n",
        "                user, item, rating = int(row[0]), int(row[1]), row[2]\n",
        "\n",
        "                pred= np.dot(pu[user,:], qi[item,:])+ global_mean + bu[user]+bi[item]\n",
        "                err = rating - pred\n",
        "\n",
        "                # Update biases\n",
        "                bu[user] += lr * (err - reg * bu[user])\n",
        "                bi[item] += lr * (err - reg * bi[item])\n",
        "\n",
        "                # Update latent factors\n",
        "                pu_update= lr * (err * qi[item, :] - reg * pu[user, :])\n",
        "                qi_update= lr * (err * pu[user, :] - reg * qi[item, :])\n",
        "                pu[user, :] = pu[user, :] + pu_update\n",
        "                qi[item, :] = qi[item, :] + qi_update\n",
        "            \n",
        "            #compute test error\n",
        "            pred = global_mean+ np.matmul(pu, qi.T) + bu.reshape(-1, 1) + bi.reshape(1, -1) \n",
        "            residual = (pred[indices] - true).ravel()\n",
        "            mse = np.square(residual).mean()\n",
        "            rmse = np.sqrt(mse)\n",
        "            mae = np.absolute(residual).mean()\n",
        "\n",
        "            #save results in df\n",
        "            self.metrics_.loc[epoch_ix, :] = (mse,rmse,mae)\n",
        "            f = lambda x: f\"{self.metrics_.loc[epoch_ix, x]:.3f}\"\n",
        "            pbar.set_postfix({f'test_{m}': f(m) for m in self.metrics_.columns})\n",
        "\n",
        "        self.bu_,self.bi_,self.pu_,self.qi_ = bu, bi, pu, qi\n",
        "        return {f\"test_{m}\": np.trim_zeros(self.metrics_[m]) for m in self.metrics_.columns}\n",
        "\n",
        "    def predict(self, X):\n",
        "        return [\n",
        "            self.predict_pair(u_id, i_id)\n",
        "            for u_id, i_id in zip(X['UserID'], X['MovieID'])\n",
        "        ]\n",
        "\n",
        "    def predict_pair(self, u_id, i_id):\n",
        "        assert u_id in self.user_mapping_ and i_id in self.item_mapping_, f\"user {u_id} or movie {i_id} not in train data\"\n",
        "        \n",
        "        i_ix = self.item_mapping_[i_id]\n",
        "        u_ix = self.user_mapping_[u_id]\n",
        "        pred = self.global_mean_ + self.bi_[i_ix] +self.bu_[u_ix] + np.dot(self.pu_[u_ix], self.qi_[i_ix])\n",
        "        pred = np.clip(pred, self.min_rating, self.max_rating)\n",
        "        return pred\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0prSFvWvuk9"
      },
      "source": [
        "class SVT:\n",
        "    def __init__(self, tau, delta, n_epochs, min_rating=1, max_rating=5):\n",
        "        self.tau = tau\n",
        "        self.delta = delta\n",
        "        self.n_epochs = n_epochs\n",
        "        self.min_rating = min_rating\n",
        "        self.max_rating = max_rating\n",
        "        self.tol = 0.001\n",
        "        self.increment = 5\n",
        "    \n",
        "    def _init_metrics(self):\n",
        "        metrics = np.zeros((self.n_epochs, 3), dtype=np.float)\n",
        "        self.metrics_ = pd.DataFrame(metrics, columns=['Loss', 'RMSE', 'MAE'])\n",
        "    \n",
        "    def fit(self, M, M_test):\n",
        "        ###preprocessing\n",
        "        \n",
        "        self._init_metrics()\n",
        "        M = M.copy()\n",
        "        M_test = M_test.copy()\n",
        "        \n",
        "        self.user_mapping_ = dict(zip(M.index, range(len(M.index))))\n",
        "        self.item_mapping_ = dict(zip(M.columns, range(len(M.columns))))\n",
        "        self.users_mean = np.mean(M.values, axis = 1).reshape(-1, 1)\n",
        "        \n",
        "        M = M.values\n",
        "        M_test= M_test.values\n",
        "        \n",
        "        \n",
        "        Omega = M.astype(np.int32).nonzero()\n",
        "        test_indices= M_test.astype(np.int32).nonzero()\n",
        "        \n",
        "        M = M- self.users_mean\n",
        "        M_test= M_test - self.users_mean\n",
        "        \n",
        "        test_true = M_test[test_indices].ravel()\n",
        "        \n",
        "        tol = self.tol\n",
        "        incre = self.increment\n",
        "        tau = self.tau\n",
        "        iterations= self.n_epochs\n",
        "        delta = self.delta\n",
        "        ###preprocessing\n",
        "\n",
        "        # SVT\n",
        "        r = 0\n",
        "        P_Omega_M = ss.csr_matrix((np.ravel(M[Omega]), Omega), shape=M.shape)\n",
        "        normProjM = norm(P_Omega_M)\n",
        "        k0 = np.ceil(tau / (delta * normProjM))\n",
        "        Y = k0 * delta * P_Omega_M\n",
        "\n",
        "        pbar = tqdm(range(iterations))\n",
        "\n",
        "        rmses = []\n",
        "        for epoch_ix in pbar:\n",
        "            s = r + 1\n",
        "            sparse_Y = ss.csc_matrix(Y)\n",
        "            \n",
        "            #find s largest eigen values. keep increasing s until the s'th largest value is smaller than tau\n",
        "            u1, s1, v1 = sparsesvd(sparse_Y, s)\n",
        "            while np.min(s1) > tau and s >= min(*M.shape):\n",
        "                u1, s1, v1 = sparsesvd(sparse_Y, s)\n",
        "                s+=incre\n",
        "            \n",
        "            #reconstruct x from svd decomposition\n",
        "            r = np.sum(s1 > tau)\n",
        "            U = u1.T[:, :r]\n",
        "            V = v1[:r, :]\n",
        "            S = s1[:r] - tau\n",
        "            x = (U * S).dot(V)\n",
        "\n",
        "            x_omega = ss.csr_matrix((x[Omega], Omega), shape=M.shape)\n",
        "            \n",
        "            #if there is no reconstruction error, stop\n",
        "            reconstrcution_loss = norm(x_omega - P_Omega_M) / norm(P_Omega_M)\n",
        "            if reconstrcution_loss < tol:\n",
        "                break\n",
        "            \n",
        "            #update Y with values from constructed matrix\n",
        "            diff = P_Omega_M - x_omega\n",
        "            Y += delta * diff\n",
        "                \n",
        "            #compute test error\n",
        "            residual = (x[test_indices].ravel() - test_true).ravel()\n",
        "            mse = np.square(residual).mean()\n",
        "            rmse = np.sqrt(mse)\n",
        "            mae = np.absolute(residual).mean()\n",
        "\n",
        "            #save results in df\n",
        "            self.metrics_.loc[epoch_ix, :] = (mse,rmse,mae)\n",
        "            f = lambda x: f\"{self.metrics_.loc[epoch_ix, x]:.3f}\"\n",
        "            pbar.set_postfix({f'test_{m}': f(m) for m in self.metrics_.columns})\n",
        "                \n",
        "        self.x = x + self.users_mean\n",
        "        return {f\"test_{m}\": np.trim_zeros(self.metrics_[m]) for m in self.metrics_.columns}\n",
        "    \n",
        "    \n",
        "    def predict(self, X):\n",
        "        return [self.predict_pair(u_id, i_id) for u_id, i_id in zip(X['UserID'], X['MovieID'])]\n",
        "\n",
        "    def predict_pair(self, u_id, i_id):\n",
        "        assert u_id in self.user_mapping_ and i_id in self.item_mapping_, f\"user {u_id} or movie {i_id} not in train data\"\n",
        "        \n",
        "        i_ix = self.item_mapping_[i_id]\n",
        "        u_ix = self.user_mapping_[u_id]\n",
        "        \n",
        "        pred= self.x[u_ix,i_ix]\n",
        "        pred = np.clip(pred, self.min_rating, self.max_rating)\n",
        "        return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbSnEbZlvuk-"
      },
      "source": [
        "## Experimental Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "vfXchdoCr7Z5"
      },
      "source": [
        "def rmse(y_true, y_pred):\n",
        "    return mse(y_true, y_pred, squared=False)\n",
        "\n",
        "def to_sparse(df):\n",
        "    users =[]\n",
        "    movies =[]\n",
        "    ratings =[]\n",
        "    indices = df.values.astype(np.int32).nonzero()\n",
        "    x_indices,y_indices = indices[0],indices[1]\n",
        "    \n",
        "    for i,j in zip(x_indices,y_indices):\n",
        "        users.append(df.index[i])\n",
        "        movies.append(df.columns[j])\n",
        "        ratings.append(df.iloc[i,j])\n",
        "    \n",
        "    df = pd.DataFrame(dict(zip(['UserID', 'MovieID', 'Rating'], [users, movies, ratings])))\n",
        "    return df\n",
        "\n",
        "def run_funk(train, test, **kw):\n",
        "    seed_everything()\n",
        "    svd = FunkSVD(**kw)\n",
        "    hist = svd.fit(X=train, X_test=test)\n",
        "    y_true, y_pred = test['Rating'].values.ravel(), np.array(svd.predict(test)).ravel()\n",
        "    print(f\"\\n\\nTest RMSE: {rmse(y_true, y_pred):.3f}\\n\\n\")\n",
        "    print_hparams(kw, 'funk')\n",
        "    return hist, svd\n",
        "    \n",
        "\n",
        "def run_svt(train, test, **kw):\n",
        "    seed_everything()\n",
        "    model = SVT(**kw)\n",
        "    hist = model.fit(M=train, M_test=test)\n",
        "    \n",
        "    sparse_test = to_sparse(test)\n",
        "    y_true, y_pred = sparse_test['Rating'].values.ravel(), np.array(model.predict(sparse_test)).ravel()\n",
        "    \n",
        "    print(f\"\\n\\nTest RMSE: {rmse(y_true, y_pred):.3f}\\n\\n\")\n",
        "    print_hparams(kw, 'SVT')\n",
        "    return hist, model\n",
        "\n",
        "def print_hparams(h, name):\n",
        "    print(\"Hyperparameters:\\n\")\n",
        "    print(f\"hist = {name}(train_df, test_df,\\n\\t\" + \", \".join([f\"{k}={h[k]}\" for k in h]) + ')')\n",
        "    \n",
        "def plot_experiment(hist):\n",
        "    fig = px.line(pd.DataFrame(hist).rename_axis('Epoch'))\\\n",
        "        .update_layout(hoverlabel=dict(font_size=12, font_family=\"Rockwell\"),\n",
        "                       font=dict(family=\"Courier New, monospace\", size=18))\\\n",
        "        .update_xaxes(showspikes=True)\n",
        "    fig.show()\n",
        "    fig.show(\"svg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gX6d_1ivuk_"
      },
      "source": [
        "## Run Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ebGMzw4vulA"
      },
      "source": [
        "hist, model = run_svt(train=M_train_df, test=M_test_df,\n",
        "\t\ttau=20000,\n",
        "\t\tdelta=2,\n",
        "\t\tn_epochs=250)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HX6ndzgpfNCi"
      },
      "source": [
        "plot_experiment(hist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSqEPw0RvulF"
      },
      "source": [
        "hist, model = run_funk(train=train_df, test=test_df,\n",
        "\t\tlr=0.01,\n",
        "\t\treg=0.005,\n",
        "\t\tn_epochs=28,\n",
        "\t\tn_factors=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6tkVLuYdfw6"
      },
      "source": [
        "plot_experiment(hist)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}